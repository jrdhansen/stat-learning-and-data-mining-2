fhat <- (1/n) * term
return(fhat)
}
fhat.i <- function(point, X, exclude.i, h){
# calculates leave one out kernel density estiamate.
# Used by outlierScore1().
# Args:
#   point: point for which density is to be estimated.
#   X: training data
#   exclude.i: index for point which is to be excluded
#   h: bandwidth parameter.
term <- 0
n <- length(X)
for(i in 1:n){
if(i != exclude.i){
val <- gaussKernel(vec = (point - X[i]), h = h)
term <- term + val
}
}
fhat <- (1/(n-1)) * term
return(fhat)
}
outlierScore1 <- function(point, X, h){
# calculates OutlierScore_1 for point.
# Args:
#   point: point for which outlier score is to be calculated.
#   X: training data
#   h: bandwidth parameter.
n <- length(X)
numerator <- kdePoint(point = point, X = X, h = h)
denominator <- 0
for(i in 1:n){
term<- fhat.i(point = X[i], X = X, exclude.i = i, h = h)
denominator <- term + denominator
}
denominator <- (1/n) * denominator
return(numerator/denominator)
}
getKnn <- function(point, X, k){
# Finds k nearest neighbors and distances. Used by outlierScore2()
# Args:
#   point: point for which k nearest neighbors are to be obtained.
#   X: training data
#   k: number of nearest neighbors to get
pointvec <- rep(point, length(X))
dist <- abs(X - pointvec)
df <- data.frame(X = X, dist = dist)
df <- arrange(df, dist)
knn <- slice(df, 1:k)
return(knn)
}
outlierScore2 <- function(point, X, k){
# calculates OultierScore_2 for point
# Args:
#   point: point for which OutlierScore_2 is to be estimated.
#   X: training data
#   k: number of nearest neighbors to calculate
knn <- getKnn(point, X, k)
numerator <- knn$dist[k]
denominator <- (1/k) * sum(knn$dist)
return(numerator/denominator)
}
################################################################################
#### To Run ####################################################################
################################################################################
# grid of possible bandwidths
grid <- seq(.01,.5, by = .01)
gridn <- length(grid)
# calculate objective function from LS-LOOCV for each value in grid
hs <- data.frame(index = 1:gridn, h = grid, objF = rep(0, gridn))
for(i in 1:nrow(hs)){
objF <- estBandWidth(df = train, h = hs$h[i])
hs$index[i] <- i
hs$objF[i] <- objF
}
# plot objective function by h
plot(hs$h, hs$objF)
# get h for which objective function is minimized.
min.h <- hs[hs$objF == min(hs$objF),]
hhat <- min.h$h
# hhat is optimal value of h.
hhat
densRange <- seq(-2, 4, by = 0.01)
# estimate density
densEst <- kdeRange(range = densRange, X = train$X, h = hhat)
# plot estimated density
plot(densEst$x, densEst$density, type = 'l', main = "Estimated Density", xlab = "X", ylab = "density")
# calculate OutlierScore_1 for xtest1 and xtest2
os1.tp1 <- outlierScore1(point = tp1, X = train$X, h = 0.08)
os1.tp2 <- outlierScore1(point = tp2, X = train$X, h = 0.08)
os1.tp1
os1.tp2
# calculate OutlierScore_2 for xtest1 and xtest2, with various values of k
os2.tp1.100 <- outlierScore2(point = tp1, X = train$X, k = 100)
os2.tp1.150 <- outlierScore2(point = tp1, X = train$X, k = 150)
os2.tp1.200 <- outlierScore2(point = tp1, X = train$X, k = 200)
os2.tp1.100
os2.tp1.150
os2.tp1.200
os2.tp2.100 <- outlierScore2(point = tp2, X = train$X, k = 100)
os2.tp2.150 <- outlierScore2(point = tp2, X = train$X, k = 150)
os2.tp2.200 <- outlierScore2(point = tp2, X = train$X, k = 200)
os2.tp2.100
os2.tp2.150
os2.tp2.200
# trainDataing is in trainData$X
df <- R.matlab::readMat("anomaly.mat") %>% lapply(t) %>% lapply(as_tibble)
trainData <- as.data.frame(df$X)
head(trainData)
trainData$index <- rownames(trainData)
tp1
df$xtest1
df$xtest5$V1
#================================================================
#==== CODE FOR STATISTICAL LEARNING II, hw5 PROBLEM 3 ===========
#================================================================
# Necessary libraries
library(geometry)
library(tidyr)
library(dplyr)
# Set the working directory
setwd("C:/Users/jrdha/OneDrive/Desktop/USU_Fa2018/Moon__SLDM2/hw5/problem4")
# trainDataing is in trainData$X
given_data <- R.matlab::readMat("anomaly.mat") %>% lapply(t) %>% lapply(as_tibble)
trainData <- as.data.frame(given_data$X)
names(trainData)[1] <- 'X'
trainData$index <- rownames(trainData)
# test points 1 and 2
testPt_1 <- given_data$xtest1$V1
testPt_2 <- given_data$xtest2$V1
################################################################################
#### Function Definitions ######################################################
################################################################################
g_Kernel <- function(vector, h){
# calculates gaussian kernel for a given vector of values.
# Args:
#   vector: vector of numeric values for which the kernel is to be estimated.
#   h: bandwidth parameter.
vec_len <- length(vector)
vector.h <- vector/h
if(vec_len > 1){
kern <- ((2*pi)^(-vec_len/2))*exp(-0.5*(dot(vector.h, vector.h)))
} else {
kern <- ((2*pi)^(-vec_len/2))*exp(-0.5*(vector.h * vector.h))
}
return((h^(-vec_len)) * kern)
}
est_BandWidth <- function(given_data, h){
# calculates objective function for LS-LOOCV
# Args:
#   given_data: data frame of training data (trainData) containing index and X .
#   h: bandwidth parameter.
data_len <- nrow(given_data)
t1 <- 0
t2 <- 0
ind <- given_data$index
X <- given_data$X
for(i in 1:data_len){
for(j in 1:data_len){
newTerm1 <- g_Kernel(vector = (X[i] - X[j]), h = (sqrt(2)*h))
t1 <- t1 + newTerm1
if(j != i){
newTerm2 <- g_Kernel(vector = (X[i] - X[j]), h = h)
t2 <- t2 + newTerm2
}
}
}
t1 <- (1/(data_len^2)) * t1
t2 <- (2/(data_len*(data_len-1))) * t2
result <- t1 - t2
return(result)
}
KDE_all_pts <- function(range, X, h){
# calculates kernel density estimate for points in range. Calls KDE_one_pt for
# all values in range.
# Args:
#   range: vector of numeric values for which the KDE is to be calculated
#   X: training data
#   h: bandwidth parameter.
#
density_estimate <- data.frame(x = range, density = double(length(range)))
for(i in 1:length(range)){
density_estimate$density[i] <- KDE_one_pt(point = range[i], X = X, h = h)
}
return(density_estimate)
}
KDE_one_pt <- function(point, X, h){
# calculates kernel desnsity estimate for point. Called by KDE_all_pts
# Args:
#   point: point for which density is to be estimated.
#   X: training data
#   h: bandwidth parameter.
term <- 0
n <- length(X)
for(i in 1:n){
val <- g_Kernel(vector = (X[i] - point), h = h)
term <- term + val
}
f_hat <- (1/n) * term
return(f_hat)
}
f_hat.i <- function(point, X, exclude.i, h){
# calculates leave one out kernel density estiamate.
# Used by outlierScore1().
# Args:
#   point: point for which density is to be estimated.
#   X: training data
#   exclude.i: index for point which is to be excluded
#   h: bandwidth parameter.
term <- 0
n <- length(X)
for(i in 1:n){
if(i != exclude.i){
val <- g_Kernel(vector = (point - X[i]), h = h)
term <- term + val
}
}
f_hat <- (1/(n-1)) * term
return(f_hat)
}
outlierScore1 <- function(point, X, h){
# calculates OutlierScore_1 for point.
# Args:
#   point: point for which outlier score is to be calculated.
#   X: training data
#   h: bandwidth parameter.
n <- length(X)
numer_frac <- KDE_one_pt(point = point, X = X, h = h)
denom_frac <- 0
for(i in 1:n){
term<- f_hat.i(point = X[i], X = X, exclude.i = i, h = h)
denom_frac <- term + denom_frac
}
denom_frac <- (1/n) * denom_frac
return(numer_frac/denom_frac)
}
findKNN_distances <- function(point, X, k){
# Finds k nearest neighbors and distances. Used by outlierScore2()
# Args:
#   point: point for which k nearest neighbors are to be obtained.
#   X: training data
#   k: number of nearest neighbors to get
pointvector <- rep(point, length(X))
distance <- abs(X - pointvector)
given_data <- data.frame(X = X, distance = distance)
given_data <- arrange(given_data, distance)
KNN <- slice(given_data, 1:k)
return(KNN)
}
outlierScore2 <- function(point, X, k){
# calculates OultierScore_2 for point
# Args:
#   point: point for which OutlierScore_2 is to be estimated.
#   X: training data
#   k: number of nearest neighbors to calculate
KNN <- findKNN_distances(point, X, k)
numer_frac <- KNN$distance[k]
denom_frac <- (1/k) * sum(KNN$distance)
return(numer_frac/denom_frac)
}
################################################################################
#### To Run ####################################################################
################################################################################
# grid of possible bandwidths
grid_list <- seq(.01,.5, by = .01)
len_grid <- length(grid_list)
# calculate objective function from LS-LOOCV for each value in grid_list
h_vals <- data.frame(index = 1:len_grid, h = grid_list, objF = rep(0, len_grid))
for(i in 1:nrow(h_vals)){
objF <- est_BandWidth(given_data = trainData, h = h_vals$h[i])
h_vals$index[i] <- i
h_vals$objF[i] <- objF
}
# plot objective function by h
plot(h_vals$h, h_vals$objF)
# get h for which objective function is minimized.
min_h <- h_vals[h_vals$objF == min(h_vals$objF),]
h_hat <- min_h$h
# h_hat is optimal value of h.
h_hat
density_range <- seq(-2, 4, by = 0.01)
# estimate density
density_estimate <- KDE_all_pts(range = density_range, X = trainData$X, h = h_hat)
# plot the estimated density obtained via KDE
plot(density_estimate$x, density_estimate$density,
type = 'l', main = "Kernel Density Estimate Plot", xlab = "X values", ylab = "density")
# calculate OutlierScore_1 for xtest1 and xtest2
os1.testPt_1 <- outlierScore1(point = testPt_1, X = trainData$X, h = 0.08)
os1.testPt_2 <- outlierScore1(point = testPt_2, X = trainData$X, h = 0.08)
os1.testPt_1
os1.testPt_2
# calculate OutlierScore_2 for xtest1 and xtest2, with various values of k
os2.testPt_1.100 <- outlierScore2(point = testPt_1, X = trainData$X, k = 100)
os2.testPt_1.150 <- outlierScore2(point = testPt_1, X = trainData$X, k = 150)
os2.testPt_1.200 <- outlierScore2(point = testPt_1, X = trainData$X, k = 200)
os2.testPt_1.100
os2.testPt_1.150
os2.testPt_1.200
os2.testPt_2.100 <- outlierScore2(point = testPt_2, X = trainData$X, k = 100)
os2.testPt_2.150 <- outlierScore2(point = testPt_2, X = trainData$X, k = 150)
os2.testPt_2.200 <- outlierScore2(point = testPt_2, X = trainData$X, k = 200)
os2.testPt_2.100
os2.testPt_2.150
os2.testPt_2.200
# h_hat is optimal value of h.
h_hat
# plot the estimated density obtained via KDE
plot(density_estimate$x, density_estimate$density,
type = 'l', main = "Kernel Density Estimate Plot, h = 0.08", xlab = "X values", ylab = "density")
# estimate density
density_estimate <- KDE_all_pts(range = density_range, X = trainData$X, h = 0.2)
# plot the estimated density obtained via KDE
plot(density_estimate$x, density_estimate$density,
type = 'l', main = "Kernel Density Estimate Plot, h = 0.08", xlab = "X values", ylab = "density")
# plot the estimated density obtained via KDE
plot(density_estimate$x, density_estimate$density,
type = 'l', main = "Kernel Density Estimate Plot, h = 0.2", xlab = "X values", ylab = "density")
#===============================================================================
#==== CODE FOR STATISTICAL LEARNING II, hw5 PROBLEM 3 ==========================
#===============================================================================
# Necessary libraries
library(geometry)
library(tidyr)
library(dplyr)
# Set the working directory
setwd("C:/Users/jrdha/OneDrive/Desktop/USU_Fa2018/Moon__SLDM2/hw5/problem4")
# trainData is in trainData$X
given_data <- R.matlab::readMat("anomaly.mat") %>% lapply(t) %>% lapply(as_tibble)
trainData <- as.data.frame(given_data$X)
names(trainData)[1] <- 'X'
trainData$index <- rownames(trainData)
# Store the given test points 1 and 2
testPt_1 <- given_data$xtest1$V1
testPt_2 <- given_data$xtest2$V1
#===============================================================================
#==== ALL FUNCTIONS ============================================================
#===============================================================================
# This function returns (calculating first) the Gaussian kernel for the vector
# of values passed in.
# Takes as arguments: vector (self-explanatory), h is the bandwidth parameter.
g_Kernel <- function(vector, h){
vec_len <- length(vector)
vector.h <- vector/h
if(vec_len > 1){
kernel_val <- ((2*pi)^(-vec_len/2))*exp(-0.5*(dot(vector.h, vector.h)))
} else {
kernel_val <- ((2*pi)^(-vec_len/2))*exp(-0.5*(vector.h * vector.h))
}
return((h^(-vec_len)) * kernel_val)
}
# This function calculates (and returns) the value of the objective function for
# the data that is passed in via LS-LOOCV using a given bandwidth.
# Takes as arguments: given_data (self-explanatory), h is the bandwidth param.
est_BandWidth <- function(given_data, h){
data_len <- nrow(given_data)
t1 <- 0
t2 <- 0
ind <- given_data$index
X <- given_data$X
# Loop through all data
for(i in 1:data_len){
for(j in 1:data_len){
newTerm1 <- g_Kernel(vector = (X[i] - X[j]), h = (sqrt(2)*h))
t1 <- t1 + newTerm1
if(j != i){
newTerm2 <- g_Kernel(vector = (X[i] - X[j]), h = h)
t2 <- t2 + newTerm2
}
}
}
t1 <- (1/(data_len^2)) * t1
t2 <- (2/(data_len*(data_len-1))) * t2
final_val <- t1 - t2
return(final_val)
}
# This function returns (after calculating) the KDE for all points in a given
# range. It calls the KDE_one_pt function for each of these points.
# Takes as arguments: range (range of values for which KDE is calculated),
# X (the training dataframe), h (bandwidth parameter).
KDE_all_pts <- function(range, X, h){
density_estimate <- data.frame(x = range, density = double(length(range)))
for(i in 1:length(range)){
density_estimate$density[i] <- KDE_one_pt(point = range[i], X = X, h = h)
}
return(density_estimate)
}
# This function returns (after calculating) the KDE for a single point.
# Takes as arguments: point (the point for which KDE is being estimated),
# X (training dataframe), h (bandwidth parameter).
KDE_one_pt <- function(point, X, h){
term <- 0
len_data <- length(X)
for(i in 1:len_data){
val <- g_Kernel(vector = (X[i] - point), h = h)
term <- term + val
}
f_hat <- (1/len_data) * term
return(f_hat)
}
# This function returns (after calculating) the LOO (leave one out) KDE, and is
# used by the outlierScore_1 function.
# Takes as arguments: point (point on which we're calculating KDE), X (training
# dataframe), exclude.i (index indicating which single point is to be excluded),
# h (bandwidth).
f_hat.i <- function(point, X, exclude.i, h){
term <- 0
n <- length(X)
for(i in 1:n){
if(i != exclude.i){
val <- g_Kernel(vector = (point - X[i]), h = h)
term <- term + val
}
}
f_hat <- (1/(n-1)) * term
return(f_hat)
}
# This function returns (after calculating) the OutlierScore_1 that is defined
# in part (c) of problem 4.
# Takes as arguments: point (calculating OutlierScore_1 for this point),
# X (training dataframe), h (bandwidth).
outlierScore_1 <- function(point, X, h){
len_data <- length(X)
numer_frac <- KDE_one_pt(point = point, X = X, h = h)
denom_frac <- 0
for(i in 1:len_data){
term<- f_hat.i(point = X[i], X = X, exclude.i = i, h = h)
denom_frac <- term + denom_frac
}
denom_frac <- (1/len_data) * denom_frac
return(numer_frac/denom_frac)
}
# This function returns (after calculating) an object, KNN, that contains the K
# nearest neighbors to a point, and the distances of each of these neighbors to
# that point.
# Takes as arguments: point (finding KNN and their distances for this point),
# X (training dataframe), k (the "k" in KNN: number of neighbors).
findKNN_distances <- function(point, X, k){
pointvector <- rep(point, length(X))
distance <- abs(X - pointvector)
given_data <- data.frame(X = X, distance = distance)
given_data <- arrange(given_data, distance)
KNN <- slice(given_data, 1:k)
return(KNN)
}
# This function returns (after calculating) the OutlierScore_2 that is defined
# in part (e) of problem 4.
# Takes as arguments: point (calculating OutlierScore_1 for this point),
# X (training dataframe), k (the "k" in KNN: number of neighbors).
outlierScore_2 <- function(point, X, k){
KNN <- findKNN_distances(point, X, k)
numer_frac <- KNN$distance[k]
denom_frac <- (1/k) * sum(KNN$distance)
return(numer_frac/denom_frac)
}
#===============================================================================
#===== FUNCTION CALLS, CODE THAT GIVES ANSWERS TO QUESTIONS IN PROBLEM 4 =======
#===============================================================================
# Define a grid of bandwidths to search through.
grid_list <- seq(.01,.5, by = .01)
len_grid <- length(grid_list)
# Using LS-LOOCV w/Gaussian kerenel calculate objective function for
# each value in grid_list
h_vals <- data.frame(index = 1:len_grid, h = grid_list, objF = rep(0, len_grid))
for(i in 1:nrow(h_vals)){
objF <- est_BandWidth(given_data = trainData, h = h_vals$h[i])
h_vals$index[i] <- i
h_vals$objF[i] <- objF
}
# Get the bandwith parameter h for which objective function is minimized (h_hat)
min_h <- h_vals[h_vals$objF == min(h_vals$objF),]
h_hat <- min_h$h
h_hat
# Define a sequence such that the KDE representation looks smooth (granularity).
density_range <- seq(-2, 4, by = 0.01)
# Estimate the density.
density_estimate <- KDE_all_pts(range = density_range,
X = trainData$X,
h = h_hat)
# Plot the estimated density obtained via KDE
plot(density_estimate$x,
density_estimate$density,
type = 'l',
main = "Kernel Density Estimate Plot, h = 0.2",
xlab = "X values",
ylab = "density")
# OutlierScore_1 for xtest1 and xtest2 using optimal bandwith parameter h
os1_testPt_1 <- outlierScore_1(point = testPt_1, X = trainData$X, h = 0.08)
os1_testPt_2 <- outlierScore_1(point = testPt_2, X = trainData$X, h = 0.08)
os1_testPt_1
os1_testPt_2
# OutlierScore_2 for xtest1 with values of k specified in the prompt
os2_testPt_1.100 <- outlierScore_2(point = testPt_1, X = trainData$X, k = 100)
os2_testPt_1.150 <- outlierScore_2(point = testPt_1, X = trainData$X, k = 150)
os2_testPt_1.200 <- outlierScore_2(point = testPt_1, X = trainData$X, k = 200)
os2_testPt_1.100
os2_testPt_1.150
os2_testPt_1.200
# OutlierScore_2 for xtest2, with values of k specified in the prompt
os2_testPt_2.100 <- outlierScore_2(point = testPt_2, X = trainData$X, k = 100)
os2_testPt_2.150 <- outlierScore_2(point = testPt_2, X = trainData$X, k = 150)
os2_testPt_2.200 <- outlierScore_2(point = testPt_2, X = trainData$X, k = 200)
os2_testPt_2.100
os2_testPt_2.150
os2_testPt_2.200
os1_testPt_1
os1_testPt_2
os2_testPt_1.100
os2_testPt_1.150
os2_testPt_1.200
os2_testPt_2.100
os2_testPt_2.150
os2_testPt_2.200

fullData$transactionRevenue.NA <- ifelse(is.na(fullData$transactionRevenue), 1, 0)
# Convert all the NAs to 0s, leave the actual values if not NA
fullData$transactionRevenue <- ifelse(is.na(fullData$transactionRevenue),
0, fullData$transactionRevenue)
# Index of the column of the transactionRevenue.NA indicator variable
colOfResp <- grep("transactionRevenue.NA", colnames(fullData))
# The downSample function requires a FACTOR variable and the indicator is numeric.
fullData$transactionRevenue.NA <- as.factor(fullData$transactionRevenue.NA)
############################
### Kaggle Test Cleaning ###
############################
# Read in the training data
testData <- read_csv("C:/Users/jrdha/OneDrive/Desktop/USU_Fa2018/Adele__BigData/_Kaggle/Data/test_flat.csv")
#testData <- read_csv("~/Stat/Big Data USU/Kaggle/test_flat.csv")
testData <- testData %>% mutate_if(is.character,as.factor)
testData <- testData %>% mutate_if(is.logical, as.factor)
# testData$transactionRevenue <- log(testData$transactionRevenue)
testData$date <- ymd(testData$date)
testData$weekday <- wday(testData$date, label=TRUE)
testData$month <- month(testData$date, label = TRUE)
# NOTE: BEFORE IMPLEMENTING THIS ON FULL DATASET, CHANGE THE DATA NAME FROM "downData"
testData$pageviews.NA             <- ifelse(is.na(testData$pageviews), 1, 0)
testData$newVisits.NA             <- ifelse(is.na(testData$newVisits), 1, 0)
testData$bounces.NA               <- ifelse(is.na(testData$bounces), 1, 0)
testData$adwordsClickInfo.page.NA <- ifelse(is.na(testData$adwordsClickInfo.page), 1, 0)
# The following columns with NAs have FACTOR variable type:
# keyword, referralPath, adContent, adwordsClickInfo.slot, adwordsClickInfo.gclId, adwordsClickInfo.adNetworkType
# === DUMMY VARIABLE CREATION FOR FEATURES OF TYPE FACTOR =====
testData$keyword.NA                        <- ifelse(is.na(testData$keyword), 1, 0)
testData$referralPath.NA                   <- ifelse(is.na(testData$referralPath), 1, 0)
testData$adContent.NA                      <- ifelse(is.na(testData$adContent), 1, 0)
testData$adwordsClickInfo.slot.NA          <- ifelse(is.na(testData$adwordsClickInfo.slot), 1, 0)
testData$adwordsClickInfo.gclId.NA         <- ifelse(is.na(testData$adwordsClickInfo.gclId), 1, 0)
testData$adwordsClickInfo.adNetworkType.NA <- ifelse(is.na(testData$adwordsClickInfo.adNetworkType), 1, 0)
# The following columns with NAs have LOGICAL variable type:
# isTrueDirect, adwordsClickInfo.isVideoAd
# === DUMMY VARIABLE CREATION FOR FEATURES OF TYPE LOGICAL =====
testData$isTrueDirect.NA               <- ifelse(is.na(testData$isTrueDirect), 1, 0)
testData$adwordsClickInfo.isVideoAd.NA <- ifelse(is.na(testData$adwordsClickInfo.isVideoAd), 1, 0)
levels(fullData$operatingSystem) <- unique(c(levels(fullData$operatingSystem), levels(testData$operatingSystem)))
levels(testData$operatingSystem) <- levels(fullData$operatingSystem)
#########################
###     Subseting     ###
#########################
fullData <- arrange(fullData, date)
index <- sample(1:700336, .3*903653)
ourTest <- fullData[index, ]
ourTrain <- fullData[-index, ]
downData <- downSample(x = ourTrain[ , -colOfResp],
y = ourTrain$transactionRevenue.NA)
#================================================#
#========== STAGE 1: CLASSIFICATION =============#
#================================================#
downData1 <- downData2 <- subset(downData,
select = - transactionRevenue)
myLevels <- lapply(downData2, function(x) length(levels(x)))
out <- 0
for(i in 1:ncol(downData1)){
if(myLevels[[i]][1] < 30){
out <- c(out, i)
}
}
predictorIndex <- out[c(2:30, 31, 35, 38, 43:57)]
predictorUsed <- colnames(downData2)[predictorIndex]
downData3 <- downData2[predictorUsed]
classifier <- randomForest(as.factor(Class) ~ .,
data = downData3,
ntree = 250)
### Predictions ###
# In bag MSE
predClassTrain <- predict(classifier, ourTrain[predictorUsed[-length(predictorUsed)]])
predClassTrain[is.na(predClassTrain)] <- 1
mean(as.numeric(ourTrain$transactionRevenue.NA) - as.numeric(predClassTrain))
# Out of bag MSE
predClassTest <- predict(classifier, ourTest[predictorUsed[-length(predictorUsed)]])
predClassTest[is.na(predClassTest)] <- 1
mean(as.numeric(ourTest$transactionRevenue.NA) - as.numeric(predClassTest))
#================================================#
#============ STAGE 2: Prediction ===============#
#================================================#
Stage2Train <- ourTrain[c("transactionRevenue",predictorUsed[-(length(predictorUsed))])]
Stage2Train <- Stage2Train[!as.logical(as.numeric(predClassTrain)-1), ]
Stage2Test <- ourTest[c("transactionRevenue", predictorUsed[-(length(predictorUsed))])]
Stage2Test <- Stage2Test[!as.logical(as.numeric(predClassTest)-1), ]
regTree <- randomForest(transactionRevenue ~ ., data = Stage2Train,
ntree = 50)
regTree.predicted <- predict(regTree, newdata = Stage2Test)
### Predictions
# In Bag MSE
trainPredictions <- 1 - (as.numeric(predClassTrain) - 1)
trainPredictions[trainPredictions == 1] <- regTree$predicted
trainPredictions[is.na(trainPredictions)] <- 0
plot(trainPredictions, ourTrain$transactionRevenue, pch = '.')
mean((trainPredictions - ourTrain$transactionRevenue)^2)
mean(ourTrain$transactionRevenue^2)
# Out of Bag MSE
testPredictions <- 1 - (as.numeric(predClassTest) - 1)
testPredictions[testPredictions == 1] <- regTree.predicted
testPredictions[is.na(testPredictions)] <- 0
plot(testPredictions, ourTest$transactionRevenue, pch = ".")
mean((testPredictions - ourTest$transactionRevenue)^2)
mean(ourTest$transactionRevenue^2)
###############################
### Kaggle Test Predictions ###
###############################
### Stage 1 ###
predClassKaggleTest <- predict(classifier, testData[predictorUsed[-length(predictorUsed)]])
predClassKaggleTest[is.na(predClassKaggleTest)] <- 1
### Stage 2 ###
Stage2KaggleTest <- testData[predictorUsed[-(length(predictorUsed))]]
Stage2KaggleTest <- Stage2KaggleTest[!as.logical(as.numeric(predClassKaggleTest)-1), ]
regTree.KagglePredicted <- predict(regTree, newdata = Stage2KaggleTest)
### Predictions ###
KaggleTestPredictions <- 1 - (as.numeric(predClassKaggleTest) - 1)
KaggleTestPredictions[KaggleTestPredictions == 1] <- regTree.KagglePredicted
KaggleTestPredictions[is.na(KaggleTestPredictions)] <- 0
###############################
#++++ Devin's Exploration ++++#
###############################
DevinTrainPredictions <- trainPredictions
DevinTrainPredictions[DevinTrainPredictions < .0] <- 0
mean((DevinTrainPredictions - ourTrain$transactionRevenue)^2)
View(KaggleTestPredictions)
?write.csv
write.csv(KaggleTestPredictions, file = "C:/Users/jrdha/OneDrive/Desktop/USU_Fa2018/Adele__BigData/_Kaggle/Data/KagglePredictions.csv)
DevinTrainPredictions <- trainPredictions
write_csv(KaggleTestPredictions, file = "C:/Users/jrdha/OneDrive/Desktop/USU_Fa2018/Adele__BigData/_Kaggle/Data/KagglePredictions.csv)
write_csv(KaggleTestPredictions, file = "C:/Users/jrdha/OneDrive/Desktop/USU_Fa2018/Adele__BigData/_Kaggle/Data/KagglePredictions.csv")
write.csv(KaggleTestPredictions, file = "C:/Users/jrdha/OneDrive/Desktop/USU_Fa2018/Adele__BigData/_Kaggle/Data/KagglePredictions.csv")
dim(testData)
plot(regTree)
varImpPlot(regTree)
is.tibble(testData)
is.tibble(ourTest)
is.tibble(ourTrain)
add_case(ourTrain, prediction = trainPredictions)
add_column(ourTrain, prediction = trainPredictions)
add_column(ourTest, prediction = testPredictions)
train
dTrain <- add_column(ourTrain, prediction = trainPredictions)
dTest <- add_column(ourTest, prediction = testPredictions)
kTest <- add_column(testData, prediction = KaggleTestPredictions)
write.csv(dTrain, file = "C:/Users/jrdha/OneDrive/Desktop/USU_Fa2018/Adele__BigData/_Kaggle/Data/dTrain.csv")
write.csv(dTest, file = "C:/Users/jrdha/OneDrive/Desktop/USU_Fa2018/Adele__BigData/_Kaggle/Data/dTest.csv")
write.csv(kTest, file = "C:/Users/jrdha/OneDrive/Desktop/USU_Fa2018/Adele__BigData/_Kaggle/Data/kTest.csv")
# L15: XML (IMDB)
# https://www.imdb.com/chart/top
# Top 250 movies as voted by our users
# Accessed on 10/29/2018
# Load the required libraries
library(httr)
library(XML)
library(magrittr)
# GET the IMDB website
doc <- GET("https://www.imdb.com/chart/top")
pagetext <- content(doc, as = "text")
class(pagetext)
cat(pagetext)
# When we look at the HTML source, we see that the movies are in a <table>.
# Let's use readHTMLTable as it converts tables into dataframes.
tabs <- readHTMLTable(pagetext)
length(tabs)
# [1] 2
# The function found two tables.
# Let's check out the dimensions of each.
sapply(tabs, dim)
#      NULL amazon-affiliates
# [1,]  250                 1
# [2,]    5                 8
# The dimensions of the first one look like the one we are after.
movieTab <- tabs[[1]]
names(movieTab)
# [1] ""             "Rank & Title" "IMDb Rating"  "Your Rating"  ""
head(movieTab)
#                                          Rank & Title IMDb Rating
# 1  1.\n      The Shawshank Redemption\n        (1994)         9.2
# 2             2.\n      The Godfather\n        (1972)         9.2
# 3    3.\n      The Godfather: Part II\n        (1974)         9.0
# 4           4.\n      The Dark Knight\n        (2008)         9.0
# 5              5.\n      12 Angry Men\n        (1957)         8.9
# 6          6.\n      Schindler's List\n        (1993)         8.9
# Your Rating
# 1 12345678910\n        \n        \n            \n            NOT YET RELEASED\n             \n            \n            Seen
# 2 12345678910\n        \n        \n            \n            NOT YET RELEASED\n             \n            \n            Seen
# 3 12345678910\n        \n        \n            \n            NOT YET RELEASED\n             \n            \n            Seen
# 4 12345678910\n        \n        \n            \n            NOT YET RELEASED\n             \n            \n            Seen
# 5 12345678910\n        \n        \n            \n            NOT YET RELEASED\n             \n            \n            Seen
# 6 12345678910\n        \n        \n            \n            NOT YET RELEASED\n             \n            \n            Seen
# Let's redo the call to readHTMLTable, this time using some of the arguments to focus in on
# exactly what we want.
top250 <- readHTMLTable(pagetext,
which = 1,
header = TRUE,
stringsAsFactors = FALSE,
colClasses = c("character", "character", "numeric", "character", "character"))
# Here we use which = 1 to say that we want only the first table in the page.
# We use header = TRUE to say that the first row of the table has variable names.
# The colClasses argument is very handy. It allows us to specify the class of the variables/columns.
# We want the third column to be converted to a numeric vector and every other column to be a character string
# (although this will be encoded as a factor by R) - so we also have to set stringsAsFactors = FALSE.
# Now let's try to extract the year from the title and create a new variable
# with this information.
# How about eliminating everything except digits from the title?
top250$Yr <- as.numeric(gsub("[^[:digit:]]", "", top250$"Rank & Title"))
head(top250$Yr)
# [1]   11994   21972   31974   42008 5121957   61993
# What happened?
# Our regular expression wasn't quite fine enough. Let's try instead to
# eliminate everything but what's between the ().
x <- gsub(".*(", "", top250$"Rank & Title")
# Error in gsub(".*(", "", top250$Title) :
#   invalid regular expression '.*(', reason 'Missing ')''
# Hmm, this is not working. Any idea why?
# The ( is a metacharacter and R is looking for the closing ).
# We need to escape it so that it is interpreted as a left parenthesis, not a metacharacter
x <- gsub(".*\\(", "", top250$"Rank & Title")
head(x)
# [1] "1994)" "1972)" "1974)" "2008)" "1957)" "1993)"
# That looks better.
# Now let's get rid of both sides of the () and change to numeric via a pipe:
top250$Yr <- gsub(".*\\(", "", top250$"Rank & Title") %>%
gsub("\\).*$", "", .) %>%
as.numeric()
head(top250$Yr)
# [1] 1994 1972 1974 2008 1957 1993
# OK, now that we have the year as a separate variable, let's eliminate it from the title.
# Also eliminate the rank, new lines, and additional spaces.
top250$Title2 <- gsub("^[[:digit:]]+\\.\n[[:blank:]]+", "", top250$"Rank & Title") %>%
gsub("\n[[:blank:]]+\\([[:digit:]]+\\)$", "", .)
head(top250$Title2)
# [1] "The Shawshank Redemption" "The Godfather"            "The Godfather: Part II"   "The Dark Knight"
# [5] "12 Angry Men"             "Schindler's List"
# Notice that I put this new variable into Title2 because
# I didn't want to wipe out the old version in case I made a mistake.
# Now that the work is done, I can drop the old versions
top250New <- top250[, c("IMDb Rating", "Title2", "Yr")]
head(top250New)
#  IMDb Rating                   Title2   Yr
#1         9.2 The Shawshank Redemption 1994
#2         9.2            The Godfather 1972
#3         9.0   The Godfather: Part II 1974
#4         9.0          The Dark Knight 2008
#5         8.9             12 Angry Men 1957
#6         8.9         Schindler's List 1993
# Let's rename the new variables.
names(top250New)
# [1] "IMDb Rating" "Title2"      "Yr"
names(top250New) <- c("Rating", "Title", "Year")
head(top250New)
#  Rating                    Title Year
#1    9.2 The Shawshank Redemption 1994
#2    9.2            The Godfather 1972
#3    9.0   The Godfather: Part II 1974
#4    9.0          The Dark Knight 2008
#5    8.9             12 Angry Men 1957
#6    8.9         Schindler's List 1993
# The data are ready.
# Let's do some statistics!
hist(top250New$Year, main = "Top-Ranked Movies by Year", xlab = "Year")
which(top250New$Year >= 1930 & top250New$Year < 1940)
# [1]  34  37  79 148 160 185 233
top250New[which(top250New$Year >= 1930 & top250New$Year < 1940), "Title"]
# [1] "City Lights"                  "Modern Times"                 "M"                            "Mr. Smith Goes to Washington"
# [5] "Gone with the Wind"           "It Happened One Night"        "The Wizard of Oz"
plot(top250New$Year, jitter(top250New$Rating, amount = 0.05),
xlab = "Release Year",
ylab = "Rating (jittered)",
main = "Ranking vs. Year")
# What is the correlation?
cor(top250New$Year, top250New$Rating)
# We might want to add other fields, e.g. genre.
# Or collect (or extract) more data...
# Note that we missed something important in the table: the number of ratings:
#   <strong title="9.2 based on 2,009,382 user ratings">9.2</strong>
#   <strong title="9.2 based on 1,376,710 user ratings">9.2</strong>
# etc.
# Extract it!
docxml <- htmlParse(doc)
xpathSApply(docxml, "//strong", xmlValue)
xpathSApply(docxml, "//strong", xmlGetAttr, "title")
xpathSApply(docxml, "//strong", xmlGetAttr, "title") %>%
unlist()
xpathSApply(docxml, "//strong", xmlGetAttr, "title") %>%
unlist() %>%
gsub("^[[:digit:]]\\.[[:digit:]] based on ", "", .)
xpathSApply(docxml, "//strong", xmlGetAttr, "title") %>%
unlist() %>%
gsub("^[[:digit:]]\\.[[:digit:]] based on ", "", .) %>%  # PEEL OFF THE STUFF BEFORE THE NUMBER
sgub(" user ratings$", "", .)
xpathSApply(docxml, "//strong", xmlGetAttr, "title") %>%
unlist() %>%
gsub("^[[:digit:]]\\.[[:digit:]] based on ", "", .) %>%  # PEEL OFF THE STUFF BEFORE THE NUMBER
gsub(" user ratings$", "", .)
xpathSApply(docxml, "//strong", xmlGetAttr, "title") %>%
unlist() %>%
gsub("^[[:digit:]]\\.[[:digit:]] based on ", "", .) %>%  # PEEL OFF THE STUFF BEFORE THE NUMBER
gsub(" user ratings$", "", .) %>%  # PEEL OFF THE STUFF AFTER THE NUMBER
gsub(",", "", .)
xpathSApply(docxml, "//strong", xmlGetAttr, "title") %>%
unlist() %>%
gsub("^[[:digit:]]\\.[[:digit:]] based on ", "", .) %>%  # PEEL OFF THE STUFF BEFORE THE NUMBER
gsub(" user ratings$", "", .) %>%  # PEEL OFF THE STUFF AFTER THE NUMBER
gsub(",", "", .) %>%  # GET RID OF THE COMMAS IN THE NUMBERS
as.numeric()
xpathSApply(docxml, "//strong", xmlGetAttr, "title") %>%
unlist() %>%
gsub("^[[:digit:]]\\.[[:digit:]] based on ", "", .) %>%  # PEEL OFF THE STUFF BEFORE THE NUMBER
gsub(" user ratings$", "", .) %>%  # PEEL OFF THE STUFF AFTER THE NUMBER
gsub(",", "", .) %>%  # GET RID OF THE COMMAS IN THE NUMBERS
as.numeric() %>%
hist(main = "Number of Ratings")
## STAT LEARNING 2
## HOMEWORK 4, Problem 6
library(caret)
library(magrittr)
library("e1071")
# Set working directory
setwd("C:/Users/jrdha/OneDrive/Desktop/USU_Fa2018/Moon__SLDM2/hw4/Problem6")
#===============================================================================
#==== INITIAL STEPS ============================================================
#===============================================================================
# Read in the data
parkData <- read.csv("parkinsonsData.csv", header = TRUE)
# Subset, get rid of first column
parkData <- parkData[ , -1]
# Split into training and test data sets. Going with the typical 80/20 split.
lengthTrain <- round(nrow(parkData) * 0.8)
lengthTest <- nrow(parkData) - lengthTrain
set.seed(1234)
# Make indices for training data subsetting
train_ind <- sample(seq_len(nrow(parkData)), size = lengthTrain)
# Split up the data
trainData <- parkData[train_ind, ]
testData <- parkData[-train_ind, ]
# Make the response a factor (rather than integer) so the algorithms perform
# classification instead of regression.
trainData$status <- as.factor(trainData$status)
#===============================================================================
#==== LOGISTIC REGRESSION ======================================================
#===============================================================================
# This specifies that we'll be doing 10-fold crossvalidation
ctrl <- trainControl(method = "repeatedcv",
number = 10,
savePredictions = TRUE)
# Train the model
mod_fit <- train(status ~.,
data = trainData,
method= "glm",
family= binomial(),
trControl = ctrl,
tuneLength = 10)
# This gives the 10-fold crossvalidated training error
trainError_logReg <- 1 - mod_fit$results$Accuracy
trainError_logReg
testData$pred = predict(mod_fit, newdata=testData)
# Generates a column that says whether or not a test observation was correctly
# classified (1==correct, 0==incorrect)
testData$correct <- with(testData, ifelse(status == pred, 1, 0))
# Generate the test error
# Total number of correct classifications
totalCorrect <- sum(testData$correct)
testError_logReg <- 1 - (totalCorrect / lengthTest)
testError_logReg
seq(1,2.1,0.01)
# THIS FITS A FINAL SVM MODEL, USING THE CROSSVALIDATED TUNING PARAMETER 1.52
svm_model <- svm(status ~., trainData, kernel = "linear", cost = 1.52)
testData$pred <- predict(svm_model, testData)
testData$correct <- with(testData, ifelse(testData$status == pred, 1, 0))
# Generate the test error
# Total number of correct classifications
totalCorrect <- sum(testData$correct)
testError_SVM <- 1 - (totalCorrect / lengthTest)
testError_SVM
# Remove the "correct" column for the SVM portion
testData <- subset(testData, select =  -c(correct, pred))
10^(-3:7)
1.0*10^(-2:2)
1e-02
1.1*10^(-2:2)
1.9*10^(-2:2)
seq(0.1,0.19,0.01)
seq(5,15,0.5)
seq(0.1,0.19,0.005)
0.1794872*39
# Libraries needed
library(caret)
library(magrittr)
library("e1071")
# Read in the data
parkData <- read.csv("parkinsonsData.csv", header = TRUE)
# Subset, get rid of first column
parkData <- parkData[ , -1]
# Split into training and test data sets. Going with the typical 80/20 split.
lengthTrain <- round(nrow(parkData) * 0.8)
lengthTest <- nrow(parkData) - lengthTrain
# Set seed
set.seed(1234)
# Make indices for training data subsetting
train_ind <- sample(seq_len(nrow(parkData)), size = lengthTrain)
# Split up the data
trainData <- parkData[train_ind, ]
testData <- parkData[-train_ind, ]
# Make the response a factor (rather than integer) so the algorithms perform
# classification instead of regression.
trainData$status <- as.factor(trainData$status)
#===============================================================================
#==== LOGISTIC REGRESSION ======================================================
#===============================================================================
# This specifies that we'll be doing 10-fold crossvalidation
ctrl <- trainControl(method = "repeatedcv",
number = 10,
savePredictions = TRUE)
# Train the model
mod_fit <- train(status ~.,
data = trainData,
method= "glm",
family= binomial(),
trControl = ctrl,
tuneLength = 10)
# This gives the 10-fold crossvalidated training error
trainError_logReg <- 1 - mod_fit$results$Accuracy
trainError_logReg
# Generates the predictions for our testData
testData$pred = predict(mod_fit, newdata=testData)
# Generates a column that says whether or not a test observation was correctly
# classified (1==correct, 0==incorrect)
testData$correct <- with(testData, ifelse(status == pred, 1, 0))
# Generate the test error
totalCorrect <- sum(testData$correct)
testError_logReg <- 1 - (totalCorrect / lengthTest)
testError_logReg
# RETURNS A TEST ERROR RATE OF 0.1794872, WHICH IS PRECISELY 7
# MISCLASSIFICATIONS. WE WILL BEAT THIS WITH BOTH KERNEL-VERSIONS OF SVM.
# Remove the "correct" and "pred" columns so that we'll have the original data
# to work with for the SVM portion
testData <- subset(testData, select =  -c(correct, pred))
set.seed(2345)
svm_tune_linear <- tune.svm(status~.,
data = trainData,
kernel = "linear",
cost = 1.9*10^(-3:3))
print(svm_tune_linear)
set.seed(2345)
svm_tune_linear <- tune.svm(status~.,
data = trainData,
kernel = "linear",
cost = seq(1,2.1,0.01))
print(svm_tune_linear)
svm_model <- svm(status ~., trainData, kernel = "linear", cost = 1.52)
testData$pred <- predict(svm_model, testData)
testData$correct <- with(testData, ifelse(testData$status == pred, 1, 0))
# Generate the test error
# Total number of correct classifications
totalCorrect <- sum(testData$correct)
testError_SVM <- 1 - (totalCorrect / lengthTest)
testError_SVM
# Remove the "correct" column for the SVM portion
testData <- subset(testData, select =  -c(correct, pred))
set.seed(2345)
svm_tune_linear <- tune.svm(status~.,
data = trainData,
kernel = "linear",
cost = seq(0.1,19,0.1))
print(svm_tune_linear)
svm_model <- svm(status ~., trainData, kernel = "linear", cost = 1.6)
testData$pred <- predict(svm_model, testData)
testData$correct <- with(testData, ifelse(testData$status == pred, 1, 0))
# Generate the test error
# Total number of correct classifications
totalCorrect <- sum(testData$correct)
testError_SVM <- 1 - (totalCorrect / lengthTest)
testError_SVM
# Remove the "correct" column for the SVM portion
testData <- subset(testData, select =  -c(correct, pred))
# INITIAL PASS (leave base values at 10)
set.seed(2345)
svm_tune_gaussian <- tune.svm(status~., data = trainData,
cost = 10^(-3:7),
gamma = 10^(-5:2))
print(svm_tune_gaussian)
set.seed(2345)
svm_tune_gaussian <- tune.svm(status~., data = trainData,
cost = 1.6,
gamma = 1.9*10^(-3:2))
print(svm_tune_gaussian)
# CHANGE THE BASE VALUES FOR COST, TRYING 1.0, 1.1, 1.2,....,1.9
set.seed(2345)
svm_tune_gaussian <- tune.svm(status~., data = trainData,
cost = 1.0*10^(-2:2),
gamma = seq(0.1,0.19,0.01))
print(svm_tune_gaussian)
set.seed(2345)
svm_tune_gaussian <- tune.svm(status~., data = trainData,
cost = seq(5,15,0.5),
gamma = seq(0.1,0.19,0.005))
print(svm_tune_gaussian)
# of C = (range of 8,9,10), and gamma = 0.105 or 0.11
costVal = 8
gammaVal = 0.105
svm_model <- svm(status ~., trainData, kernel = "radial",
cost = costVal, gamma = gammaVal)
testData$pred <- predict(svm_model, testData)
testData$correct <- with(testData, ifelse(testData$status == pred, 1, 0))
# Generate the test error
# Total number of correct classifications
totalCorrect <- sum(testData$correct)
testError_SVM <- 1 - (totalCorrect / lengthTest)
testError_SVM
# Remove the "correct" column for the SVM portion
testData <- subset(testData, select =  -c(correct, pred))
knitr::stitch('prob6_code_FINAL.R')
browseURL('prob6_code_FINAL.pdf')
